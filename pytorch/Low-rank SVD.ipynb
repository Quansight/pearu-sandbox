{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "PyTorch [PR 29488](https://github.com/pytorch/pytorch/pull/29488) implements a SVD for low-rank matrices.\n",
    "Here we compare the low-rank SVD implementation with the full SVD implementation of pytorch.\n",
    "\n",
    "## User-interfaces\n",
    "\n",
    "The low-rank and full SVD have the following user-interfaces:\n",
    "```\n",
    "torch.svd_lowrank(A, q=6, niter=2, M=None) -> (U, S, V)\n",
    "\n",
    "svd(A, some=True, compute_uv=True, out=None) -> (U, S, V)\n",
    "```\n",
    "\n",
    "## A test with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, r = 17, 5, 3  # matrix shape and rank\n",
    "A = random_lowrank_matrix(m, n, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1=tensor([0.2338, 0.1937, 0.0564], dtype=torch.float64)\n",
      "U1:\n",
      "tensor([[-0.3878, -0.4516,  0.0321],\n",
      "        [ 0.7447,  0.0968, -0.2437],\n",
      "        [-0.5172,  0.2004, -0.4313],\n",
      "        [-0.0663,  0.3149,  0.8497],\n",
      "        [-0.1525,  0.8046, -0.1777]], dtype=torch.float64)\n",
      "V1^T:\n",
      "tensor([[ 0.4575,  0.8885,  0.0343],\n",
      "        [ 0.5940, -0.3342,  0.7318],\n",
      "        [ 0.6617, -0.3144, -0.6807]], dtype=torch.float64)\n",
      "\n",
      "S2=tensor([0.2338, 0.1937, 0.0564], dtype=torch.float64)\n",
      "U2:\n",
      "tensor([[-0.3878, -0.4516, -0.0321],\n",
      "        [ 0.7447,  0.0968,  0.2437],\n",
      "        [-0.5172,  0.2004,  0.4313],\n",
      "        [-0.0663,  0.3149, -0.8497],\n",
      "        [-0.1525,  0.8046,  0.1777]], dtype=torch.float64)\n",
      "V2^T:\n",
      "tensor([[ 0.4575,  0.8885,  0.0343],\n",
      "        [ 0.5940, -0.3342,  0.7318],\n",
      "        [-0.6617,  0.3144,  0.6807]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "U1, S1, V1 = torch.svd_lowrank(A)\n",
    "print(f'S1={S1}')\n",
    "print(f'U1:\\n{U1}')\n",
    "print(f'V1^T:\\n{V1.t()}\\n')\n",
    "U2, S2, V2 = torch.svd(A)\n",
    "print(f'S2={S2}')\n",
    "print(f'U2:\\n{U2}')\n",
    "print(f'V2^T:\\n{V2.t()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, both svd methods give the same singular values and are able to detect the rank of A correctly.\n",
    "The UV matrices are also the same (up to a sign multiplier).\n",
    "\n",
    "# Timings and applicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 µs ± 4.27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "22 µs ± 56.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "308 µs ± 1.57 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "43.2 µs ± 262 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.svd_lowrank(A)\n",
    "%timeit torch.svd(A)\n",
    "A2 = random_lowrank_matrix(10000, 200, 5)\n",
    "%timeit torch.svd_lowrank(A2)\n",
    "%timeit torch.svd(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the full-rank SVD implementation out-performs the low-rank SVD implementation for dense matrices. However, the low-rank SVD implementation will be handy for huge sparse matices where the full-rank SVD fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3=tensor([1.1147e+00, 9.0710e-01, 4.6210e-01, 4.1026e-01, 2.5382e-01, 1.8031e-07],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::_svd_helper' with arguments from the 'SparseCPUTensorId' backend. 'aten::_svd_helper' is only available for these backends: [CPUTensorId, VariableTensorId].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-dc1027e14e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mU3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd_lowrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'S3={S3}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not run 'aten::_svd_helper' with arguments from the 'SparseCPUTensorId' backend. 'aten::_svd_helper' is only available for these backends: [CPUTensorId, VariableTensorId]."
     ]
    }
   ],
   "source": [
    "A3 = random_sparse_matrix(100000, 10000, density=2/10000.0)\n",
    "U3,S3,V3=torch.svd_lowrank(A3)\n",
    "print(f'S3={S3}')\n",
    "torch.svd(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "The auxiliary code used in this notebook. Run this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def random_matrix(rows, columns, *batch_dims, **kwargs):\n",
    "    \"\"\"Return rectangular matrix or batches of rectangular matrices.\n",
    "\n",
    "    Parameters:\n",
    "      dtype - the data type\n",
    "      device - the device kind\n",
    "      singular - when True, the output will be singular\n",
    "    \"\"\"\n",
    "    dtype = kwargs.get('dtype', torch.double)\n",
    "    device = kwargs.get('device', 'cpu')\n",
    "    silent = kwargs.get(\"silent\", False)\n",
    "    singular = kwargs.get(\"singular\", False)\n",
    "    if silent and not torch._C.has_lapack:\n",
    "        return torch.ones(rows, columns, dtype=dtype, device=device)\n",
    "\n",
    "    A = torch.randn(batch_dims + (rows, columns), dtype=dtype, device=device)\n",
    "    u, _, v = A.svd(some=False)\n",
    "    s = torch.zeros(rows, columns, dtype=dtype, device=device)\n",
    "    k = min(rows, columns)\n",
    "    for i in range(k):\n",
    "        s[i, i] = float(i + 1) / (k + 1)\n",
    "    if singular:\n",
    "        # make matrix singular\n",
    "        s[k - 1, k - 1] = 0\n",
    "        if k > 2:\n",
    "            # increase the order of singularity so that the pivoting\n",
    "            # in LU factorization will be non-trivial\n",
    "            s[0, 0] = 0\n",
    "    return u.matmul(s.expand(batch_dims + (rows, columns)).matmul(v.transpose(-2, -1)))\n",
    "\n",
    "\n",
    "def random_lowrank_matrix(rank, rows, columns, *batch_dims, **kwargs):\n",
    "    \"\"\"Return rectangular matrix or batches of rectangular matrices with\n",
    "    given rank.\n",
    "    \"\"\"\n",
    "    B = random_matrix(rows, rank, *batch_dims, **kwargs)\n",
    "    C = random_matrix(rank, columns, *batch_dims, **kwargs)\n",
    "    return B.matmul(C)\n",
    "\n",
    "def random_sparse_matrix(rows, columns, density=0.01, **kwargs):\n",
    "    \"\"\"Return rectangular random sparse matrix within given density.\n",
    "\n",
    "    The density of the result approaches to given density as the size\n",
    "    of the matrix is increased and a relatively small value of density\n",
    "    is specified but higher than min(rows, columns)/(rows * columns)\n",
    "    for non-singular matrices.\n",
    "    \"\"\"\n",
    "    dtype = kwargs.get('dtype', torch.double)\n",
    "    device = kwargs.get('device', 'cpu')\n",
    "    singular = kwargs.get(\"singular\", False)\n",
    "\n",
    "    k = min(rows, columns)\n",
    "    nonzero_elements = max(min(rows, columns), int(rows * columns * density))\n",
    "\n",
    "    row_indices = [i % rows for i in range(nonzero_elements)]\n",
    "    column_indices = [i % columns for i in range(nonzero_elements)]\n",
    "    random.shuffle(column_indices)\n",
    "    indices = [row_indices, column_indices]\n",
    "    values = torch.randn(nonzero_elements, dtype=dtype, device=device)\n",
    "    # ensure that the diagonal dominates\n",
    "    values *= torch.tensor([-float(i - j)**4 for i, j in zip(*indices)], dtype=dtype, device=device).exp()\n",
    "    A = torch.sparse_coo_tensor(indices, values, (rows, columns), device=device)\n",
    "    return A.coalesce()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 3",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
